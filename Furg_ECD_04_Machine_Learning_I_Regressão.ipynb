{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "Furg - ECD 04 - Machine Learning I - Regressão",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pcpiscator/01T2021/blob/main/Furg_ECD_04_Machine_Learning_I_Regress%C3%A3o.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k26Ux9sPljno"
      },
      "source": [
        "# Curso de Especialização em Ciência de Dados - FURG\n",
        "## Machine Learning I - Regressão\n",
        "### Prof. Marcelo Malheiros\n",
        "\n",
        "Código adaptado de Aurélien Geron (licença Apache-2.0)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2cTdADGljnx"
      },
      "source": [
        "# Inicialização\n",
        "\n",
        "Aqui importamos as bibliotecas fundamentais de Python para este _notebook_:\n",
        "\n",
        "- NumPy: suporte a vetores, matrizes e operações de Álgebra Linear\n",
        "- Matplotlib: biblioteca de visualização de dados\n",
        "- Scikit-Learn: biblioteca com algoritmos de Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcKwhqLVljny"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bInOEgvmljn0"
      },
      "source": [
        "# Regressão Linear\n",
        "\n",
        "Primeiro vamos criar um conjunto sintético de dados, com um \"ruído\" aleatório adicionado à reta $y = 4 + 3 * X$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xODNiXXVljn1"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2AYDLhaljn1"
      },
      "source": [
        "plt.plot(X, y, 'b.')\n",
        "plt.xlabel('$x_1$', fontsize=18)\n",
        "plt.ylabel('$y$', rotation=0, fontsize=18)\n",
        "plt.axis([0, 2, 0, 15])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Mg-pjMOljn4"
      },
      "source": [
        "## Treinamento usando a solução de forma fechada\n",
        "\n",
        "Isso quer dizer que o modelo de regressão linear vai criar uma equação completa para fazer a previsão, usando todos os dados de treinamento de uma só vez.\n",
        "\n",
        "Agora vamos treinar o modelo com as _features_ em `X` e os rótulos em `y`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8UlEycOljn5"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zUEc58Yljn5"
      },
      "source": [
        "O modelo treinado aprendeu os parâmetros abaixo, da reta original $y = 4 + 3 * X$. A discrepância se dá por conta do ruído adicionado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-QyCMwcljn6"
      },
      "source": [
        "print('interseção: ', lin_reg.intercept_[0])\n",
        "print('coeficiente:' , lin_reg.coef_[0, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXdfduySljn7"
      },
      "source": [
        "Agora vamos plotar a reta dada pelo modelo de regressão, usando o resultado das previsões para 0 e 2 (canto esquerdo e direito do gráfico, respectivamente)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF8VTAPMljn9"
      },
      "source": [
        "X_new = np.array([[0], [2]])\n",
        "y_predict = lin_reg.predict(X_new)\n",
        "\n",
        "plt.plot(X_new, y_predict, \"r-\")\n",
        "plt.plot(X, y, \"b.\")\n",
        "plt.axis([0, 2, 0, 15])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnmAyvcXljn9"
      },
      "source": [
        "## Funcionamento do método de descida do gradiente\n",
        "\n",
        "Aqui o processo é incremental, começando longe de uma boa aproximação, mas gradualmente chegando perto de uma boa solução à medida que novos dados de treinamento são processados.\n",
        "\n",
        "Este é um exemplo de um método *iterativo* de otimização.\n",
        "\n",
        "Não se preocupe com os detalhes do código abaixo, este é só para demonstrar a estrutura do método de descida do gradiente!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNfNpgs5ljn-"
      },
      "source": [
        "# taxa de aprendizado (learning rate)\n",
        "eta = 0.1  \n",
        "\n",
        "# número de passos (iterações)\n",
        "n_iterations = 1000\n",
        "\n",
        "# ponto inicial aleatório\n",
        "theta = np.random.randn(2,1)\n",
        "\n",
        "X_b = np.c_[np.ones((100, 1)), X]\n",
        "for iteration in range(n_iterations):\n",
        "    gradients = 2/100 * X_b.T.dot(X_b.dot(theta) - y)\n",
        "    theta = theta - eta * gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1anxTcbuljn-"
      },
      "source": [
        "print('interseção: ',  theta[0, 0])\n",
        "print('coeficiente:' , theta[1, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vClp8r0lljn_"
      },
      "source": [
        "O código a seguir é para gerar as figuras abaixo, que ilustram o proceso de descida do gradiente. Não se preocupe com os detalhes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpoG3fcJljn_"
      },
      "source": [
        "def plot_gradient_descent(theta, eta, theta_path=None):\n",
        "    m = len(X_b)\n",
        "    plt.plot(X, y, 'b.')\n",
        "    n_iterations = 1000\n",
        "    for iteration in range(n_iterations):\n",
        "        if iteration < 10:\n",
        "            y_predict = X_new_b.dot(theta)\n",
        "            style = 'b-' if iteration > 0 else 'r--'\n",
        "            plt.plot(X_new, y_predict, style)\n",
        "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "        theta = theta - eta * gradients\n",
        "        if theta_path is not None:\n",
        "            theta_path.append(theta)\n",
        "    plt.xlabel('$x_1$', fontsize=18)\n",
        "    plt.axis([0, 2, -2, 15])\n",
        "    plt.title('$\\eta = {}$'.format(eta), fontsize=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0Q380u-ljoA"
      },
      "source": [
        "Na figura mais à esquerda a taxa de aprendizado é $\\eta=0.02$, causando uma **convergência muito lenta**.\n",
        "\n",
        "Na figura do meio, a taxa é $\\eta=0.1$, dando a **convergência correta**.\n",
        "\n",
        "Na figura mais à direita, a taxa é muito grande com $\\eta=0.5$, o que causa **divergência**. Ou seja, o método numérico não converge para a solução desejada\n",
        "\n",
        "A linha vermelha representa o ponto inicial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_JiQWtGljoA"
      },
      "source": [
        "np.random.seed(42)\n",
        "theta = np.random.randn(2,1) # ponto inicial aleatório\n",
        "\n",
        "X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
        "plt.figure(figsize=(15,3))\n",
        "plt.subplot(131); plot_gradient_descent(theta, eta=0.02)\n",
        "plt.ylabel('$y$', rotation=0, fontsize=18)\n",
        "plt.subplot(132); plot_gradient_descent(theta, eta=0.1)\n",
        "plt.subplot(133); plot_gradient_descent(theta, eta=0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHW5eux_ljoB"
      },
      "source": [
        "## Treinamento usando o método de descida do gradiente\n",
        "\n",
        "Aqui vamos usar o algoritmo `SGDRegressor`, que randomiza a escolha de instâncias a cada passo (sendo uma técnica de Descida de Gradiente Estocástico).\n",
        "\n",
        "O parâmetro `eta0` é a taxa de aprendizado, enquando `max_iter` é o limite para o número de passos.\n",
        "\n",
        "A vantagem é poder fugir com mais facilidade de mínimos locais e encontrar os parâmetros que minimizam globalmente o erro, encontrando a melhor solução para a regressão linear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9AlqxivljoB"
      },
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "sgd_reg = SGDRegressor(eta0=0.1, penalty=None, random_state=42, max_iter=1000)\n",
        "sgd_reg.fit(X, y.ravel())\n",
        "print('interseção: ',  sgd_reg.intercept_[0])\n",
        "print('coeficiente:' , sgd_reg.coef_[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN2YM35vljoB"
      },
      "source": [
        "# Regressão polinomial\n",
        "\n",
        "Vamos criar um conjunto sintético de dados, com um \"ruído\" aleatório adicionado à curva $y = 2 + X + 0.5 X^2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrVLDz_fljoC"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "X = 6 * np.random.rand(100, 1) - 3\n",
        "y = 2 + X + 0.5 * X**2 + np.random.randn(100, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foRao9Q-ljoD"
      },
      "source": [
        "plt.plot(X, y, 'b.')\n",
        "plt.xlabel('$x_1$', fontsize=18)\n",
        "plt.ylabel('$y$', rotation=0, fontsize=18)\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM9V9RLhljoD"
      },
      "source": [
        "O trecho mostra o recurso de transformação de dados chamado `PolynomialFeatures`, que automatiza a inserção de novas _features_ no _dataset_ original.\n",
        "\n",
        "No caso, vamos inserir apenas um termo quadrático $x^2$ para cada feature $x$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x86O9I3xljoD"
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvzGXZiQljoE"
      },
      "source": [
        "print('instância #0 original:  ', X[0])\n",
        "print('instância #0 modificada:', X_poly[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwPJ-ERxljoE"
      },
      "source": [
        "Compare os parâmetros aprendidos com a equação da curva original, $y = 2 + X + 0.5 X^2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XslX4A7ljoF"
      },
      "source": [
        "# ajuste pelo algoritmo de regressão linear (usando solução de forma fechada)\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_poly, y)\n",
        "print('interseção:  ',  lin_reg.intercept_[0])\n",
        "print('coeficientes:' , lin_reg.coef_[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4CdFg_aljoF"
      },
      "source": [
        "# exibição da curva ajustada\n",
        "X_new = np.linspace(-3, 3, 100).reshape(100, 1)\n",
        "X_new_poly = poly_features.transform(X_new)\n",
        "y_new = lin_reg.predict(X_new_poly)\n",
        "plt.plot(X, y, 'b.')\n",
        "plt.plot(X_new, y_new, 'r-', linewidth=2, label='previsões')\n",
        "plt.xlabel('$x_1$', fontsize=18)\n",
        "plt.ylabel('$y$', rotation=0, fontsize=18)\n",
        "plt.legend(loc='upper left', fontsize=14)\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErykBaftljoF"
      },
      "source": [
        "Note que uma regressão polinomial é capaz de encontrar relacionamentos entre features (o que um modelo de regressão linear simples não pode fazer).\n",
        "\n",
        "Isso é possível porque `PolynomialFeatures` também pode adicionar todas as combinações de _features_ até o grau determinado. Por exemplo, se houvesse duas _features_ $a$ e $b$, `PolynomialFeatures` com `degree=3` não apenas adicionaria as _features_ $a^2$, $a^3$, $b^2$ e $b^3$, mas também as combinações $ab$, $a^2b$ e $ab^2$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjX-b5eoljoG"
      },
      "source": [
        "# Regressão logística"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aSQXFeyljoG"
      },
      "source": [
        "t = np.linspace(-10, 10, 100)\n",
        "sig = 1 / (1 + np.exp(-t))\n",
        "plt.figure(figsize=(9, 3))\n",
        "plt.plot([-10, 10], [0, 0], \"k-\")\n",
        "plt.plot([-10, 10], [0.5, 0.5], \"k:\")\n",
        "plt.plot([-10, 10], [1, 1], \"k:\")\n",
        "plt.plot([0, 0], [-1.1, 1.1], \"k-\")\n",
        "plt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\frac{1}{1 + e^{-t}}$\")\n",
        "plt.xlabel(\"t\")\n",
        "plt.legend(loc=\"upper left\", fontsize=20)\n",
        "plt.axis([-10, 10, -0.1, 1.1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTEqc2lCljoH"
      },
      "source": [
        "## Obter os dados\n",
        "\n",
        "Nesta atividade iremos usar o conjunto de dados IRIS, que é um conjunto famoso que contém o comprimento e largura das sépalas e das pétalas de 150 flores de íris de três espécies diferentes (_Iris setosa_, _Iris versicolor_ e _Iris virginica).\n",
        "\n",
        "Este _dataset_ já faz parte da biblioteca Scikit-Learn, e pode ser importado diretamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VBFTEtHljoH"
      },
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "list(iris.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECb-arLuljoH"
      },
      "source": [
        "Agora será construido um classificador usando regressão logística para detectar o tipo _Iris virginica_ com base apenas na _feature_ de largura da pétala."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il0uCyb2ljoH"
      },
      "source": [
        "X = iris['data'][:, 3:]                   # largura da pétala\n",
        "y = (iris['target'] == 2).astype(np.int)  # 1 se é Iris virginica, caso contrário 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwAy1cXrljoH"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_reg = LogisticRegression(random_state=42)\n",
        "log_reg.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On6S1e5KljoI"
      },
      "source": [
        "Um classificador logístico é baseado em probabilidades, então vamos examinar o modelo e verificar as probabilidades estimadas para flores com larguras de pétalas variando de 0 a 3 cm.\n",
        "\n",
        "Note que `log_reg.predict_proba()` devolve probabilidades no intervalo [0, 1], enquanto `log_reg.predict()` retorna a ser da classe (classe positiva ou 1) ou não ser da classe (negativa ou 0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTsLXjl0ljoI"
      },
      "source": [
        "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
        "y_proba = log_reg.predict_proba(X_new)\n",
        "limiar = X_new[y_proba[:, 1] >= 0.5][0]\n",
        "\n",
        "plt.plot(X_new, y_proba[:, 1], 'g-',  linewidth=2, label='Iris virginica')\n",
        "plt.plot(X_new, y_proba[:, 0], 'b--', linewidth=2, label='Outra')\n",
        "plt.legend(loc='center left', fontsize=14)\n",
        "plt.plot([limiar, limiar], [0, 1], 'r:', linewidth=2)\n",
        "plt.text(limiar + 0.7, 0.5, 'limiar de decisão', fontsize=14, color='r', ha='center')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgQBbPOxljoJ"
      },
      "source": [
        "val  = np.array([[1.7], [1.5]])\n",
        "prob = log_reg.predict_proba(val)\n",
        "pred = log_reg.predict(val)\n",
        "\n",
        "print('limiar: ', limiar)\n",
        "print('valor:', val[0, 0], 'probabilidades:', prob[0], 'previsão:', pred[0])\n",
        "print('valor:', val[1, 0], 'probabilidades:', prob[1], 'previsão:', pred[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQnpFIlvljoK"
      },
      "source": [
        "O trecho de código e respectiva figura abaixo mostram o mesmo conjunto de dados, mas desta vez exibindo duas características: largura e comprimento da pétala.\n",
        "\n",
        "Uma vez treinado, o classificador da regressão logística pode estimar a probabilidade de que uma nova flor seja uma _Iris virginica_ com base nessas duas características.\n",
        "\n",
        "A linha tracejada representa os pontos onde o modelo estima uma probabilidade de 50%: este é o limiar de decisão. Observe que é um limite linear. Cada linha paralela representa os pontos onde o modelo produz uma probabilidade específica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWi6PMCkljoK"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = (iris[\"target\"] == 2).astype(np.int)\n",
        "\n",
        "log_reg = LogisticRegression(C=10**10, random_state=42)\n",
        "log_reg.fit(X, y)\n",
        "\n",
        "x0, x1 = np.meshgrid(\n",
        "    np.linspace(2.9, 7, 500).reshape(-1, 1),\n",
        "    np.linspace(0.8, 2.7, 200).reshape(-1, 1))\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "\n",
        "y_proba = log_reg.predict_proba(X_new)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\")\n",
        "plt.plot(X[y==1, 0], X[y==1, 1], \"g^\")\n",
        "\n",
        "zz = y_proba[:, 1].reshape(x0.shape)\n",
        "contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\n",
        "\n",
        "left_right = np.array([2.9, 7])\n",
        "boundary = -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1]\n",
        "\n",
        "plt.clabel(contour, inline=1, fontsize=12)\n",
        "plt.plot(left_right, boundary, \"k--\", linewidth=3)\n",
        "plt.text(6.5, 2.3, \"Iris virginica\", fontsize=14, color=\"g\", ha=\"center\")\n",
        "plt.text(3.5, 1.5, \"Outra\", fontsize=14, color=\"b\", ha=\"center\")\n",
        "plt.xlabel(\"Comprimento da pétala\", fontsize=14)\n",
        "plt.ylabel(\"Largura da pétala\", fontsize=14)\n",
        "plt.axis([2.9, 7, 0.8, 2.7])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CecydsnKljoK"
      },
      "source": [
        "# Regressão softmax\n",
        "\n",
        "A ideia é bastante simples: quando é dada uma instância $\\mathbf{x}$, o modelo de regressão softmax primeiro calcula uma pontuação $s_k(\\mathbf{x})$ para cada classe $k$. Em seguida, estima a probabilidade de cada classe aplicando a função **softmax** (também chamada de exponencial normalizada) sobre as pontuações.\n",
        "\n",
        "A classificação será feita para as três espécies de plantas. A classe `LogisticRegression` passa a funcionar como um classificador softmax ao se especificar o parâmetro `multi_class='multinomial'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps0e86huljoL"
      },
      "source": [
        "X = iris['data'][:, (2, 3)]  # comprimento da pétala, largura da pétala\n",
        "y = iris['target']           # rótulos\n",
        "\n",
        "softmax_reg = LogisticRegression(multi_class='multinomial', C=10, random_state=42)\n",
        "softmax_reg.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTqMT6zmljoL"
      },
      "source": [
        "A figura abaixo ilustra as regiões de classificação para algumas instâncias, codificadas em três cores.\n",
        "\n",
        "Além disso, os limiares de probabilidade para a espécie _Iris versicolor_ são mostrados nas curvas contínuas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYZ43E0AljoL"
      },
      "source": [
        "x0, x1 = np.meshgrid(\n",
        "    np.linspace(0, 8, 500).reshape(-1, 1),\n",
        "    np.linspace(0, 3.5, 200).reshape(-1, 1),)\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "\n",
        "y_proba = softmax_reg.predict_proba(X_new)\n",
        "y_predict = softmax_reg.predict(X_new)\n",
        "\n",
        "zz1 = y_proba[:, 1].reshape(x0.shape)\n",
        "zz = y_predict.reshape(x0.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris virginica\")\n",
        "plt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris versicolor\")\n",
        "plt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris setosa\")\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
        "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
        "contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\n",
        "plt.clabel(contour, inline=1, fontsize=12)\n",
        "plt.xlabel(\"Comprimento da pétala\", fontsize=14)\n",
        "plt.ylabel(\"Largura da pétala\", fontsize=14)\n",
        "plt.legend(loc=\"center left\", fontsize=14)\n",
        "plt.axis([0, 7, 0, 3.5])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl0NJtWBljoL"
      },
      "source": [
        "val  = np.array([[4, 1]])\n",
        "prob = softmax_reg.predict_proba(val)\n",
        "pred = softmax_reg.predict(val)\n",
        "\n",
        "print('valor:         ', val[0])\n",
        "print('probabilidades:', prob)\n",
        "print('previsão:      ', pred[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}